{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import pickle as pkl\n",
    "\n",
    "import mne\n",
    "import cmlreaders as cml\n",
    "import cluster_helper.cluster\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from constants import FR2_valid_subjects, subjects_powerfile_elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LTP093 LTP106 LTP115 LTP117 LTP123 LTP133 LTP138 LTP207 LTP210 LTP228 LTP229 LTP236 LTP246 LTP249 LTP250 LTP251 LTP258 LTP259 LTP265 LTP269 LTP273 LTP278 LTP279 LTP280 LTP283 LTP285 LTP287 LTP293 LTP296 LTP297 LTP299 LTP301 LTP302 LTP303 LTP304 LTP305 LTP306 LTP307 LTP310 LTP311 LTP312 LTP316 LTP317 LTP318 LTP321 LTP322 LTP323 LTP324 LTP325 LTP326 LTP327 LTP328 LTP329 LTP331 LTP334 LTP336 LTP339 LTP341 LTP342 LTP343 LTP344 LTP346 LTP347 LTP348 LTP349 LTP354 LTP355 LTP357 uh oh LTP357 /scratch/liyuxuan/ltpFR2/mtpower/LTP357/LTP357_ret_logmtpowerts_3.pkl 263\n",
      "uh oh LTP357 /scratch/liyuxuan/ltpFR2/mtpower/LTP357/LTP357_ret_logmtpowerts_6.pkl 135\n",
      "uh oh LTP357 /scratch/liyuxuan/ltpFR2/mtpower/LTP357/LTP357_enc_logmtpowerts_3.pkl 263\n",
      "uh oh LTP357 /scratch/liyuxuan/ltpFR2/mtpower/LTP357/LTP357_enc_logmtpowerts_6.pkl 135\n",
      "LTP360 uh oh LTP360 scratch/mtpower/LTP360/LTP360_enc_logmtpowerts_3.pkl 135\n",
      "uh oh LTP360 scratch/mtpower/LTP360/LTP360_enc_logmtpowerts_7.pkl 263\n",
      "uh oh LTP360 /scratch/liyuxuan/ltpFR2/mtpower/LTP360/LTP360_ret_logmtpowerts_3.pkl 135\n",
      "uh oh LTP360 /scratch/liyuxuan/ltpFR2/mtpower/LTP360/LTP360_ret_logmtpowerts_7.pkl 263\n",
      "LTP361 uh oh LTP361 scratch/mtpower/LTP361/LTP361_enc_logmtpowerts_4.pkl 135\n",
      "uh oh LTP361 scratch/mtpower/LTP361/LTP361_enc_logmtpowerts_9.pkl 135\n",
      "uh oh LTP361 scratch/mtpower/LTP361/LTP361_enc_logmtpowerts_12.pkl 135\n",
      "uh oh LTP361 /scratch/liyuxuan/ltpFR2/mtpower/LTP361/LTP361_ret_logmtpowerts_4.pkl 135\n",
      "uh oh LTP361 /scratch/liyuxuan/ltpFR2/mtpower/LTP361/LTP361_ret_logmtpowerts_9.pkl 135\n",
      "uh oh LTP361 /scratch/liyuxuan/ltpFR2/mtpower/LTP361/LTP361_ret_logmtpowerts_12.pkl 135\n",
      "LTP362 LTP364 LTP365 uh oh LTP365 /scratch/liyuxuan/ltpFR2/mtpower/LTP365/LTP365_ret_logmtpowerts_14.pkl 135\n",
      "uh oh LTP365 /scratch/liyuxuan/ltpFR2/mtpower/LTP365/LTP365_enc_logmtpowerts_14.pkl 135\n",
      "LTP366 uh oh LTP366 /scratch/liyuxuan/ltpFR2/mtpower/LTP366/LTP366_ret_logmtpowerts_0.pkl 135\n",
      "uh oh LTP366 /scratch/liyuxuan/ltpFR2/mtpower/LTP366/LTP366_enc_logmtpowerts_0.pkl 135\n",
      "LTP367 LTP371 LTP372 LTP373 LTP374 LTP376 LTP377 LTP385 LTP386 LTP387 LTP389 LTP390 LTP391 LTP393 "
     ]
    }
   ],
   "source": [
    "problematic_sessions = [] # ['LTP357','LTP360','LTP361','LTP365','LTP366']\n",
    "for s in FR2_valid_subjects:\n",
    "    print(s, end=' ')\n",
    "    nchan = 128 if int(s[-3:]) > 330 else 124\n",
    "    if s=='LTP360' or s=='LTP361':\n",
    "        files = glob.glob('scratch/mtpower/%s/%s_enc_logmtpowerts_*.pkl'%(s,s)) + glob.glob('/scratch/liyuxuan/ltpFR2/mtpower/%s/%s_ret_logmtpowerts_*.pkl'%(s,s))\n",
    "    else:\n",
    "        files = glob.glob('/scratch/liyuxuan/ltpFR2/mtpower/%s/%s_*_logmtpowerts_*.pkl'%(s,s))\n",
    "    for f in files:\n",
    "        x = pkl.load(open(f,'rb'))\n",
    "        if len(x.channels.values)!=nchan:\n",
    "            print('uh oh', s, f, len(x.channels.values))\n",
    "            problematic_sessions.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_power(subject):\n",
    "    \n",
    "    session = int(str.split(subject,'-')[1])\n",
    "    subject = str.split(subject,'-')[0]\n",
    "    \n",
    "    task_phase_indicator = 'ret' # 'enc' or 'ret'\n",
    "    \n",
    "    save_dir = 'scratch/mtpower/' # '/scratch/liyuxuan/ltpFR2/mtpower/'\n",
    "    \n",
    "    import os\n",
    "    import glob\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import xarray as xr\n",
    "    import pickle as pkl\n",
    "    \n",
    "    import mne\n",
    "    import cmlreaders as cml\n",
    "    \n",
    "    from constants import frequencies as freqs_keep, all_biosemi_channels\n",
    "    \n",
    "    # timepoints of interest\n",
    "    # 500ms window\n",
    "    # 50ms step size\n",
    "    if task_phase_indicator == 'enc':\n",
    "        # covering 250ms ~ 1350ms covering the encoding window of 0~1600ms\n",
    "        timepoints = np.arange(250, 1350+1, 50)\n",
    "    if task_phase_indicator == 'ret':\n",
    "        timepoints = np.arange(-750, 0+1, 50)\n",
    "    moving_window_size = 500 # ms\n",
    "    \n",
    "    def process_raw_eeg(subject, session):\n",
    "        \n",
    "        eegpath = '/protocols/ltp/subjects/%s/experiments/ltpFR2/sessions/%d/ephys/current_processed/' % (subject, session)\n",
    "        sys = 'bio' if int(subject[-3:]) > 330 else 'egi'\n",
    "\n",
    "        if sys=='egi':\n",
    "            eegfile = glob.glob(eegpath+'*.2.raw') + glob.glob(eegpath+'*.1.raw') + glob.glob(eegpath+'*.mff')\n",
    "            if len(eegfile)!=1: return\n",
    "            eegfile = eegfile[0]\n",
    "            raw = mne.io.read_raw_egi(eegfile, preload=True)\n",
    "            raw.rename_channels({'E129': 'Cz'})\n",
    "            raw.set_montage(mne.channels.read_montage('GSN-HydroCel-129'))\n",
    "            raw.set_channel_types({'E8': 'eog', 'E25': 'eog', 'E126': 'eog', 'E127': 'eog', 'Cz': 'misc'})\n",
    "        if sys=='bio':\n",
    "            eegfile = glob.glob(eegpath+'*.bdf')\n",
    "            if len(eegfile)!=1: return\n",
    "            eegfile = eegfile[0]\n",
    "            raw = mne.io.read_raw_edf(eegfile, \n",
    "                                      eog=['EXG1', 'EXG2', 'EXG3', 'EXG4'],\n",
    "                                      misc=['EXG5', 'EXG6', 'EXG7', 'EXG8'],\n",
    "                                      stim_channel='Status',\n",
    "                                      montage='biosemi128',\n",
    "                                      preload=True) # needs to be true for 0.1Hz high-pass filter to work\n",
    "        \n",
    "        badchanfiles = glob.glob(eegpath+'*_bad_chan[0-2].txt')\n",
    "        if len(badchanfiles) > 0:\n",
    "            bad = []\n",
    "            for bcf in badchanfiles:\n",
    "                with open(bcf, 'r') as f:\n",
    "                    bad = bad + [s.strip() for s in f.readlines()]\n",
    "            raw.info['bads'] = bad\n",
    "\n",
    "        # high-pass filter\n",
    "        raw.filter(l_freq=0.1, h_freq=None) # fir\n",
    "        # line noise filter\n",
    "        raw.filter(62.0, 58.0, method='iir', iir_params=dict(ftype='butter', order=4, output='sos'))\n",
    "        \n",
    "        return raw\n",
    "    \n",
    "    def compute_session_power(subject, session):\n",
    "        \n",
    "        reader = cml.CMLReader(subject=subject, \n",
    "                               experiment='ltpFR2', \n",
    "                               session=session)\n",
    "        events = reader.load('events')\n",
    "        if task_phase_indicator == 'enc':\n",
    "            events = events[events['type']=='WORD']\n",
    "        if task_phase_indicator == 'ret':\n",
    "            events = events[events['type']=='REC_WORD']\n",
    "        \n",
    "        raw = process_raw_eeg(subject, session)\n",
    "        \n",
    "        # compute power at each timepoint of interest\n",
    "        # -- ensuring matching returning frequencies\n",
    "        power_allinterval = []\n",
    "        for t in timepoints:\n",
    "        \n",
    "            mne_events = np.zeros((len(events), 3), dtype=int)\n",
    "            mne_events[:, 0] = [o for i, o in enumerate(events['eegoffset'])]\n",
    "            epochs = mne.Epochs(raw, mne_events, \n",
    "                                tmin=(t-moving_window_size/2)/1000.0, \n",
    "                                tmax=(t+moving_window_size/2)/1000.0+0.2, \n",
    "                                baseline=None, preload=True, on_missing='ignore')\n",
    "\n",
    "            epochs._data = epochs._data * 1000000 # convert to microvolts\n",
    "            \n",
    "            # epochs.pick_types(eeg=True, exclude=[])\n",
    "            # ^ doesn't work for some Biosemi sessions with montage set to weird settings when recording\n",
    "            # replaing this line with the following to explicitly select the subset of channels we want before avg ref\n",
    "            epochs.pick_channels(ch_names=all_biosemi_channels)\n",
    "            \n",
    "            # use custom avg reference so bad channels are excluded in computing avg but still referenced\n",
    "            channel_avg = epochs.copy().pick_types(eeg=True, exclude='bads')._data.mean(1)\n",
    "            epochs._data = epochs._data - np.repeat(np.expand_dims(channel_avg, 1), \n",
    "                                                    len(epochs.info['ch_names']), \n",
    "                                                    axis=1)\n",
    "            epochs.resample(500.0) # resample to 500hz\n",
    "            \n",
    "            tminind = 0 # np.where(np.isclose(epochs.times, t/1000-moving_window_size/1000/2))[0][0]\n",
    "            tmaxind = tminind + 250 # 0.5s in 500hz space --> 250 samples\n",
    "            x = mne.EpochsArray(epochs.get_data()[:,:,tminind:tmaxind], epochs.info, verbose=False)\n",
    "\n",
    "            x.info['bads'] = [] # keep bad channels in computing power\n",
    "            power, fdone = mne.time_frequency.psd_multitaper(x, \n",
    "                                                             fmin=2.0, \n",
    "                                                             fmax=128.0, \n",
    "                                                             verbose=False)\n",
    "            power = xr.DataArray(power,\n",
    "                                 dims=('events','channels','frequency'),\n",
    "                                 coords={'events':events.to_records() if type(events)!=np.recarray else events,\n",
    "                                         'channels':epochs.info['ch_names'],\n",
    "                                         'frequency':fdone})\n",
    "            power = power.sel(frequency=freqs_keep)\n",
    "            power_allinterval.append(power)\n",
    "\n",
    "        # concat into times x events x channels x frequencies\n",
    "        power = xr.concat(power_allinterval, dim='time')\n",
    "        power.coords['time'] = timepoints\n",
    "        del events, epochs\n",
    "\n",
    "        # post-power-computation processing\n",
    "        power = np.log10(power)\n",
    "        power.values = power.values.astype(np.float32)\n",
    "        \n",
    "        return power\n",
    "    \n",
    "    path = save_dir+'%s/' % subject\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    \n",
    "    outf = save_dir + '%s/%s_%s_logmtpowerts_%d.pkl' % (subject, subject, task_phase_indicator, session)\n",
    "    power = compute_session_power(subject, session)\n",
    "    pkl.dump(power, open(outf, 'wb'))\n",
    "    \n",
    "#     for session in range(24):\n",
    "        \n",
    "#         outf = save_dir + '%s/%s_%s_logmtpowerts_%d.pkl' % (subject, subject, task_phase_indicator, session)\n",
    "        \n",
    "#         if os.path.exists(outf):\n",
    "#             continue\n",
    "        \n",
    "#         try:\n",
    "#             power = compute_session_power(subject, session)\n",
    "            \n",
    "#             # save this session\n",
    "#             # pkl.dump(power, open(outf, 'wb'))\n",
    "#             print(outf)\n",
    "#             return power\n",
    "        \n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "#             continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Engines running\n"
     ]
    }
   ],
   "source": [
    "# subjects = FR2_valid_subjects\n",
    "# print('computing power for ', subjects, '......')\n",
    "# njobs = 30\n",
    "# cpj = 4\n",
    "\n",
    "problematic_sessions = ['LTP357-3','LTP357-6',\n",
    "                        'LTP360-3','LTP360-7',\n",
    "                        'LTP361-4','LTP361-9','LTP361-12',\n",
    "                        'LTP365-14',\n",
    "                        'LTP366-0']\n",
    "njobs = len(problematic_sessions)\n",
    "cpj = 4\n",
    "\n",
    "with cluster_helper.cluster.cluster_view(scheduler='sge',\n",
    "                                         queue='RAM.q', \n",
    "                                         num_jobs=njobs, \n",
    "                                         cores_per_job=cpj) as view:\n",
    "    view.map(compute_power, problematic_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_power(subject):\n",
    "    \n",
    "    # global flag\n",
    "    task = 'ret' # enc or ret\n",
    "    \n",
    "    import os\n",
    "    import glob\n",
    "    import numpy as np\n",
    "    import xarray as xr\n",
    "    import pickle as pkl\n",
    "    \n",
    "    def special_concat(arr_of_ts):\n",
    "        '''\n",
    "        Can be used to concatenate timeseries object that contains the CML events dimension\n",
    "        To get around the problem when PTSA concatenates multiple timeseries object the events dim\n",
    "        will turn out to have dype='O'\n",
    "        Assumes the timeseries objects to have a 'events' dim\n",
    "        '''\n",
    "        if len(arr_of_ts)==1:\n",
    "            return arr_of_ts[0]\n",
    "        else:\n",
    "            x = xr.concat(arr_of_ts, dim='events')\n",
    "            events = arr_of_ts[0].events.values\n",
    "            for ts in arr_of_ts[1:]:\n",
    "                events = np.concatenate((events, ts.events.values))\n",
    "            x.coords['events'] = events\n",
    "            return x\n",
    "\n",
    "    def filter_recalls(x):\n",
    "        '''x is assumed to be a recall EEG timeseries\n",
    "           filters x based on some recall exclusion criteria\n",
    "        '''\n",
    "        events = x.events.values\n",
    "        keep_index = np.zeros(len(events), dtype=bool)\n",
    "\n",
    "        # read events in again to get REC_VV vocalization events\n",
    "        reader = cml.CMLReader(subject=np.unique(events['subject'])[0], \n",
    "                               experiment='ltpFR2',\n",
    "                               session=np.unique(events['session'])[0])\n",
    "        original_events = reader.load('events')\n",
    "        original_events = original_events[(original_events['type']=='REC_WORD') | (original_events['type']=='REC_WORD_VV')]\n",
    "\n",
    "        for trial in np.unique(events['trial']):\n",
    "            trial_recs = events[events['trial']==trial]\n",
    "            trial_recs_vvs = original_events[original_events['trial']==trial]\n",
    "\n",
    "            # get timebefore w.r.t. onset of last recall for rec_word events\n",
    "            rectimes = trial_recs_vvs['rectime']\n",
    "            timebefore = np.diff(np.append([0], rectimes))\n",
    "            timebefore = timebefore[trial_recs_vvs['type']=='REC_WORD']\n",
    "\n",
    "            trial_valid_rec_flag = np.ones(len(trial_recs), dtype=bool)\n",
    "            trial_valid_rec_flag[timebefore<1000] = 0\n",
    "            \n",
    "            keep_index[events['trial']==trial] = trial_valid_rec_flag\n",
    "\n",
    "        x = x.sel(events=keep_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "    if task == 'enc':\n",
    "    \n",
    "        files = glob.glob('scratch/mtpower/%s/*_enc_logmtpowerts*.pkl' % subject)\n",
    "        # files =glob.glob('/scratch/liyuxuan/ltpFR2/mtpower/%s/*_enc_logmtpowerts*.pkl' % subject)\n",
    "\n",
    "        power_all = []\n",
    "        for f in files:\n",
    "            \n",
    "            try:\n",
    "                x = pkl.load(open(f,'rb'))\n",
    "                \n",
    "                # technically covers 0 ~ 1600\n",
    "                x = x.sel(time=(x.time>=250)&(x.time<=1350)).mean('time')\n",
    "\n",
    "                means = x.mean('events')\n",
    "                stds = x.std('events')\n",
    "                x = (x-means)/stds\n",
    "\n",
    "                power_all.append(x)\n",
    "        \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "        \n",
    "        power_all = special_concat(power_all)\n",
    "        outf = 'scratch/mtpower/%s/%s_enc_zpower.pkl' % (subject, subject)\n",
    "        # outf = '/scratch/liyuxuan/ltpFR2/mtpower/%s/%s_enc_zpower.pkl' % (subject, subject)\n",
    "        pkl.dump(power_all, open(outf, 'wb'))\n",
    "    \n",
    "    if task == 'ret':\n",
    "        \n",
    "        files = glob.glob('scratch/mtpower/%s/*_ret_logmtpowerts*.pkl' % subject)\n",
    "        # files = glob.glob('/scratch/liyuxuan/ltpFR2/mtpower/%s/*_ret_logmtpowerts*.pkl' % subject)\n",
    "        \n",
    "        power_all = []\n",
    "        for f in files:\n",
    "\n",
    "            try:\n",
    "                x = pkl.load(open(f,'rb'))\n",
    "                \n",
    "                # preserve events with no overlap w/ vocalization only\n",
    "                x = filter_recalls(x)\n",
    "                \n",
    "                # select from time -0.25s\n",
    "                x = x.sel(time=-250)\n",
    "                \n",
    "                means = x.mean('events')\n",
    "                stds = x.std('events')\n",
    "                x = (x-means)/stds\n",
    "\n",
    "                power_all.append(x)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "        \n",
    "        power_all = special_concat(power_all)\n",
    "        outf = 'scratch/mtpower/%s/%s_ret_zpower.pkl' % (subject, subject)\n",
    "        # outf = '/scratch/liyuxuan/ltpFR2/mtpower/%s/%s_ret_zpower.pkl' % (subject, subject)\n",
    "        pkl.dump(power_all, open(outf, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LTP093\n",
      "LTP106\n",
      "LTP115\n",
      "LTP117\n",
      "LTP123\n",
      "LTP133\n",
      "LTP138\n",
      "LTP207\n",
      "LTP210\n",
      "LTP228\n",
      "LTP229\n",
      "LTP236\n",
      "LTP246\n",
      "LTP249\n",
      "LTP250\n",
      "LTP251\n",
      "LTP258\n",
      "LTP259\n",
      "LTP265\n",
      "LTP269\n",
      "LTP273\n",
      "LTP278\n",
      "LTP279\n",
      "LTP280\n",
      "LTP283\n",
      "LTP285\n",
      "LTP287\n",
      "LTP293\n",
      "LTP296\n",
      "LTP297\n",
      "LTP299\n",
      "LTP301\n",
      "LTP302\n",
      "LTP303\n",
      "LTP304\n",
      "LTP305\n",
      "LTP306\n",
      "LTP307\n",
      "LTP310\n",
      "LTP311\n",
      "LTP312\n",
      "LTP316\n",
      "LTP317\n",
      "LTP318\n",
      "LTP321\n",
      "LTP322\n",
      "LTP323\n",
      "LTP324\n",
      "LTP325\n",
      "LTP326\n",
      "LTP327\n",
      "LTP328\n",
      "LTP329\n",
      "LTP331\n",
      "LTP334\n",
      "LTP336\n",
      "LTP339\n",
      "LTP341\n",
      "LTP342\n",
      "LTP343\n",
      "LTP344\n",
      "LTP346\n",
      "LTP347\n",
      "LTP348\n",
      "LTP349\n",
      "LTP354\n",
      "LTP355\n",
      "LTP357\n",
      "LTP360\n",
      "LTP361\n",
      "LTP362\n",
      "LTP364\n",
      "LTP365\n",
      "LTP366\n",
      "LTP367\n",
      "LTP371\n",
      "LTP372\n",
      "LTP373\n",
      "LTP374\n",
      "LTP376\n",
      "LTP377\n",
      "LTP385\n",
      "LTP386\n",
      "LTP387\n",
      "LTP389\n",
      "LTP390\n",
      "LTP391\n",
      "LTP393\n"
     ]
    }
   ],
   "source": [
    "for s in FR2_valid_subjects:\n",
    "    print(s,)\n",
    "    concat_power(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LTP357\n",
      "LTP360\n",
      "LTP361\n",
      "LTP365\n",
      "LTP366\n"
     ]
    }
   ],
   "source": [
    "for s in ['LTP357', 'LTP360', 'LTP361', 'LTP365', 'LTP366']:\n",
    "    print(s,)\n",
    "    concat_power(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PTSA3.6",
   "language": "python",
   "name": "ptsa3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
